# -*- coding: utf-8 -*-
"""Machine learning applied to drug discovery (Balancing)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JQiWHDj4OauR-gQW15mHvikWxWko5cYy

Mounting the drive and importing the libraries:
"""

# Import necessary libraries for mounting the drive
from google.colab import drive

# Mount the Google drive
drive.mount('/content/drive')

# Import other necessary libraries
import os
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import f1_score
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
import glob
from sklearn.model_selection import StratifiedKFold
from sklearn.utils import class_weight
from imblearn.pipeline import Pipeline
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils import resample
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.model_selection import KFold, train_test_split











# Download the file
!gdown --id 1bmdrvVub6pqimcITpOpAZKrvP8K9POM9

"""#**SMOTE AND UNDERSAMPLING**
This code evaluates the performance of a Random Forest classifier using various combinations of Synthetic Minority Over-sampling Technique **(SMOTE)** and **undersampling** strategies on multiple datasets. The datasets are first loaded and split into training (90%) and testing (10%) sets. Although a test set is created, it's not used in this analysis.

For each dataset, a 5-fold stratified cross-validation is performed on the training data. In each fold, the data is further split into a learning set and a validation set. Based on different combinations of SMOTE and undersampling strategies, the learning set is balanced, and a Random Forest classifier is trained. The classifier's performance is then evaluated on the validation set using the macro-averaged F1-score.
"""

# Define the path to the datasets
datasets_path = '/content/drive/MyDrive/'

# Define the strategy values, add 0.0 for no resampling
strategy_values = [i/10. for i in range(11)]  # [0.0, 0.1, 0.2, ..., 1.0]

# Define Random Forest Classifier
rf = RandomForestClassifier()

# Fetch all files starting with 'ind_' and end with '_frame.tsv'
files = [f for f in os.listdir(datasets_path) if f.startswith('ind_') and f.endswith('_frame.tsv')]

# Loop over each file
for file in files:
    print(f"Processing file: {file}")

    # Load the dataset
    df = pd.read_csv(os.path.join(datasets_path, file), sep='\t')

    # Split the data into 90% training and 10% testing
    X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:-1], df.iloc[:, -1], test_size=0.10, random_state=42)

    # Save the training and testing datasets
    df.iloc[X_train.index].to_csv(os.path.join(datasets_path, f'training_{file}'), sep='\t', index=False)
    df.iloc[X_test.index].to_csv(os.path.join(datasets_path, f'testing_{file}'), sep='\t', index=False)

    # Create a dataframe to save the results
    results_df = pd.DataFrame(columns=['SMOTE_strategy', 'Undersampling_strategy', 'NumPosExamp', 'NumNegExamp', 'Average_F1_score'])

    # Define a 5-fold cross-validation
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    # Loop over each SMOTE and undersampling strategy value
    for smote_strategy in strategy_values:
        for under_strategy in strategy_values:

            f1_scores = []  # Initialize the list to store F1-scores for each fold

            # Cross-validation loop
            for train_idx, valid_idx in skf.split(X_train, y_train):
                X_learning, X_validation = X_train.iloc[train_idx], X_train.iloc[valid_idx]
                y_learning, y_validation = y_train.iloc[train_idx], y_train.iloc[valid_idx]

                # Initialize SMOTE and RandomUnderSampler if strategy is not 0
                sm = SMOTE(sampling_strategy=smote_strategy) if smote_strategy > 0 else None
                rus = RandomUnderSampler(sampling_strategy=under_strategy) if under_strategy > 0 else None

                try:
                    # Fit and resample with SMOTE and RandomUnderSampler
                    X_res, y_res = X_learning, y_learning  # start with original data
                    if sm:
                        X_res, y_res = sm.fit_resample(X_res, y_res)
                    if rus:
                        X_res, y_res = rus.fit_resample(X_res, y_res)

                    # Count the number of positive and negative examples
                    counts = np.bincount(y_res)
                    num_pos_examp = counts[1]
                    num_neg_examp = counts[0]

                    # Fit Random Forest Classifier on the resampled learning data
                    rf.fit(X_res, y_res)

                    # Predict on the validation data
                    predictions = rf.predict(X_validation)

                    # Calculate F1-score
                    f1 = f1_score(y_validation, predictions, average='macro')
                    f1_scores.append(f1)

                except Exception as e:
                    print(f"Exception caught during processing of fold: {e}")
                    continue

            # Calculate the average F1-score over all folds
            average_f1_score = np.mean(f1_scores)

            # Add the results to the results dataframe
            results_df = results_df.append({
                'SMOTE_strategy': smote_strategy,
                'Undersampling_strategy': under_strategy,
                'NumPosExamp': num_pos_examp,
                'NumNegExamp': num_neg_examp,
                'Average_F1_score': average_f1_score,
            }, ignore_index=True)

            # Print the average F1-score for each strategy combination
            print(f"For {file}, SMOTE strategy {smote_strategy}, undersampling strategy {under_strategy}, Average F1-score is {average_f1_score}")

    # Save the results dataframe to a CSV file
    results_df.to_csv(f'{datasets_path}The_First_results_of_SMOTE_Undersampling_{file.split(".")[0]}.csv', index=False)

"""  ** the following code:**

   processes the results of machine learning experiments (validation) that utilized different combinations of SMOTE and undersampling strategies. The steps are as follows:

    Matrix Creation: For each result file, the script reads the data and pivots it to create a matrix representation. This matrix showcases the average F1-scores for each combination of SMOTE and undersampling ratios. The matrix is then saved as a new CSV file.

    Maximum F1-score Identification: The script reads each matrix file to identify the highest F1-score and the corresponding SMOTE and undersampling ratios that achieved this score. These details are stored in a new DataFrame.

    TSV File Association: For each matrix file, the script identifies the corresponding original .tsv dataset file. If any .tsv file is missing, a warning is printed.

    Results Saving: The final results, including the matrix file names, maximum F1-scores, and the associated SMOTE and undersampling ratios, are saved to a CSV file for further analysis.
"""

# The path where the result files are stored
path = '/content/drive/MyDrive/'

# Get the list of all result files
result_files = glob.glob(f"{path}The_First_results_of_SMOTE_Undersampling_*.csv")

# Process each file individually
for file in result_files:
    # Read the result file into a DataFrame
    df = pd.read_csv(file)

    # Pivot the DataFrame to create a matrix, use the average F1-score as values
    matrix = df.pivot_table(index='SMOTE_strategy', columns='Undersampling_strategy', values='Average_F1_score')

    # Replace the index name with "SMOTE ratios | Undersampling ratios"
    matrix.index.name = "SMOTE ratios | Undersampling ratios"

    # Determine the output filename by replacing "Stage1_results" with "Matrix_results" in the original filename
    output_file = file.replace('The_First_results_of_SMOTE_Undersampling_', 'The_Matrix_result_of_')

    # Save the matrix to a CSV file, replace NaN values with 'NA'
    matrix.to_csv(output_file, na_rep='NA', index=True)





# Get the list of all matrix files
matrix_files = glob.glob(f"{path}The_Matrix_result_of_*.csv")

# Create an empty DataFrame to store the results
results_df = pd.DataFrame(columns=['Matrix_File', 'Max_F1_Score', 'SMOTE_Ratio', 'Undersampling_Ratio'])

# Process each matrix file individually
for file in matrix_files:
    # Read the matrix file into a DataFrame
    df = pd.read_csv(file, index_col=0)

    # Find the maximum F1 score and its corresponding SMOTE and Undersampling ratios
    max_f1 = df.max().max()  # First max gets the maximum value per column, second max gets the overall maximum
    smote_ratio, undersampling_ratio = df.stack().idxmax()

    # Append the results to the results DataFrame
    results_df = results_df.append({
        'Matrix_File': file.split('/')[-1],  # Extracting only the filename
        'Max_F1_Score': max_f1,
        'SMOTE_Ratio': smote_ratio,
        'Undersampling_Ratio': undersampling_ratio
    }, ignore_index=True)

# Save the results to a CSV file
results_df.to_csv(f"{path}Max_F1_Scores_Results.csv", index=False)



# Read the Max_F1_Scores_Results.csv file
results_df = pd.read_csv(f"{path}Max_F1_Scores_Results.csv")

# Create a list to store the corresponding .tsv files
tsv_files = []

# For each matrix file in the results DataFrame, identify the corresponding .tsv file
for matrix_file in results_df['Matrix_File']:
    # Extract the relevant part of the matrix file name
    file_part = matrix_file.replace('The_Matrix_result_of_', '').replace('.csv', '')

    # Construct the corresponding .tsv file name
    tsv_file = f"{path}{file_part}.tsv"

    # Check if the .tsv file exists
    if os.path.exists(tsv_file):
        tsv_files.append(tsv_file)
    else:
        print(f"Warning: {tsv_file} does not exist!")
        tsv_files.append(None)
print(tsv_files)

# Optionally, save the updated results DataFrame to a new CSV file
results_df.to_csv(f"{path}Updated_Max_F1_Scores_Results.csv", index=False)

"""**TESTING:**

this code evaluates the performance of a Random Forest Classifier on multiple datasets using previously determined optimal SMOTE and undersampling ratios. It begins by loading a results file, which has information about each dataset's optimal resampling ratios. For each dataset, a 10-fold cross-validation is conducted. In each fold, the data is first resampled using the optimal SMOTE and undersampling ratios, then a Random Forest Classifier is trained on the resampled training set, and predictions are made on the test set. Metrics such as F1 score, recall, and precision are computed for these predictions. After processing all datasets, the average metrics across all folds for each dataset are saved in a new file, "Final_SMOTE_Undersampling_Evaluation_Results.csv".
"""

# Load the updated results file
results_df = pd.read_csv('/content/drive/MyDrive/Updated_Max_F1_Scores_Results.csv')

# Define Random Forest Classifier
rf = RandomForestClassifier()

# Store results for each file in a list
results = []

# STEP 1: Extract the corresponding file name and the ratios from each row
for _, row in results_df.iterrows():
    file_name = row['Matrix_File'].replace('The_Matrix_result_of_', '').replace('.csv', '.tsv')
    smote_ratio = row['SMOTE_Ratio']
    undersampling_ratio = row['Undersampling_Ratio']

    # STEP 2: Run a 10-fold Random Forest using the extracted ratios on the corresponding file
    df = pd.read_csv(f"/content/drive/MyDrive/{file_name}", sep='\t')
    X = df.iloc[:, 1:-1]
    y = df.iloc[:, -1]

    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    f1_scores = []
    recall_scores = []
    precision_scores = []

    for train_idx, test_idx in skf.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        # Conditionally apply SMOTE and RandomUnderSampler
        X_res, y_res = X_train, y_train
        if smote_ratio > 0:
            sm = SMOTE(sampling_strategy=smote_ratio)
            X_res, y_res = sm.fit_resample(X_res, y_res)
        if undersampling_ratio > 0:
            rus = RandomUnderSampler(sampling_strategy=undersampling_ratio)
            X_res, y_res = rus.fit_resample(X_res, y_res)

        # Train and predict with Random Forest
        rf.fit(X_res, y_res)
        predictions = rf.predict(X_test)

        # Store scores
        f1_scores.append(f1_score(y_test, predictions, average='macro'))
        recall_scores.append(recall_score(y_test, predictions, average='macro'))
        precision_scores.append(precision_score(y_test, predictions, average='macro'))

    # STEP 3: Store average scores and file details for each file
    results.append({
        'File_Name': file_name,
        'SMOTE_Ratio': smote_ratio,
        'Undersampling_Ratio': undersampling_ratio,
        'Recall': sum(recall_scores) / len(recall_scores),
        'Precision': sum(precision_scores) / len(precision_scores),
        'F1': sum(f1_scores) / len(f1_scores)
    })

# Convert results to DataFrame and save to CSV
final_results_df = pd.DataFrame(results)
final_results_df.to_csv('/content/drive/MyDrive/Final_SMOTE_Undersampling_Evaluation_Results.csv', index=False)

"""#**SMOTE AND CLASSWEIGHT**

This code evaluates the performance of a Random Forest classifier using various combinations of Synthetic Minority Over-sampling Technique (SMOTE) and classweight strategies on multiple datasets. The datasets are first loaded and split into training (90%) and testing (10%) sets. Although a test set is created, it's not used in this analysis.

For each dataset, a 5-fold stratified cross-validation is performed on the training data. In each fold, the data is further split into a learning set and a validation set. Based on different combinations of SMOTE and classweight strategies, the learning set is balanced, and a Random Forest classifier is trained. The classifier's performance is then evaluated on the validation set using the macro-averaged F1-score.
"""

# Define the path to the datasets
datasets_path = '/content/drive/MyDrive/'

# Define the SMOTE ratios
smote_ratios = [i/10. for i in range(11)]  # [0.0, 0.1, 0.2, ..., 1.0]

# Define the class weights
class_weights = [i for i in range(1, 11)]  # [1, 2, ..., 10]

# Fetch all files starting with 'ind_' and end with '_frame.tsv'
files = [f for f in os.listdir(datasets_path) if f.startswith('ind_') and f.endswith('_frame.tsv')]

# Loop over each file
for file in files:
    print(f"Processing file: {file}")

    # Load the dataset
    df = pd.read_csv(os.path.join(datasets_path, file), sep='\t')

    # Split the data into 90% training and 10% testing
    X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 1:-1], df.iloc[:, -1], test_size=0.10, random_state=42)

    # Create a dataframe to save the results
    results_df = pd.DataFrame(columns=['SMOTE_ratio', 'ClassWeight', 'NumPosExamp', 'NumNegExamp', 'Average_F1_score'])

    # Define a 5-fold cross-validation
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    # Loop over each SMOTE ratio and class weight value
    for smote_ratio in smote_ratios:
        for class_weight_value in class_weights:

            # Initialize class weight dictionary
            class_weight = {0: 1, 1: class_weight_value}

            f1_scores = []  # Initialize the list to store F1-scores for each fold

            # Cross-validation loop
            for train_idx, valid_idx in skf.split(X_train, y_train):
                X_learning, X_validation = X_train.iloc[train_idx], X_train.iloc[valid_idx]
                y_learning, y_validation = y_train.iloc[train_idx], y_train.iloc[valid_idx]

                # Initialize SMOTE if ratio is not 0
                sm = SMOTE(sampling_strategy=smote_ratio) if smote_ratio > 0 else None

                try:
                    # Fit and resample with SMOTE
                    X_res, y_res = X_learning, y_learning  # start with original data
                    if sm:
                        X_res, y_res = sm.fit_resample(X_res, y_res)

                    # Count the number of positive and negative examples
                    counts = np.bincount(y_res)
                    num_pos_examp = counts[1]
                    num_neg_examp = counts[0]

                    # Fit Random Forest Classifier on the resampled learning data
                    rf = RandomForestClassifier(class_weight=class_weight)
                    rf.fit(X_res, y_res)

                    # Predict on the validation data
                    predictions = rf.predict(X_validation)

                    # Calculate F1-score
                    f1 = f1_score(y_validation, predictions, average='macro')
                    f1_scores.append(f1)

                except Exception as e:
                    print(f"Exception caught during processing of fold: {e}")
                    continue

            # Calculate the average F1-score over all folds
            average_f1_score = np.mean(f1_scores)

            # Add the results to the results dataframe
            results_df = results_df.append({
                'SMOTE_ratio': smote_ratio,
                'ClassWeight': class_weight_value,
                'NumPosExamp': num_pos_examp,
                'NumNegExamp': num_neg_examp,
                'Average_F1_score': average_f1_score,
            }, ignore_index=True)

            # Print the average F1-score for each strategy combination
            print(f"For {file}, SMOTE ratio {smote_ratio}, class weight {class_weight_value}, Average F1-score is {average_f1_score}")

    # Save the results dataframe to a CSV file
    results_df.to_csv(f'{datasets_path}The_First_results_of_SMOTE_Classweight_{file.split(".")[0]}.csv', index=False)

"""This script processes the results of machine learning experiments that utilized different combinations of SMOTE and class weights. The steps are as follows:

    Matrix Creation: For each result file, the script reads the data and pivots it to create a matrix representation. This matrix showcases the average F1-scores for each combination of SMOTE ratios and class weights. The matrix is then saved as a new CSV file.

    Maximum F1-score Identification: The script reads each matrix file to identify the highest F1-score and the corresponding SMOTE ratio and class weight that achieved this score. These details are stored in a new DataFrame.

    TSV File Association: For each matrix file, the script identifies the corresponding original .tsv dataset file. If any .tsv file is missing, a warning is printed.

    Results Saving: The final results, including the matrix file names, maximum F1-scores, and the associated SMOTE ratios and class weights, are saved to a CSV file for further analysis.


"""

# The path where the result files are stored
path = '/content/drive/MyDrive/'

# Get the list of all result files
result_files = glob.glob(f"{path}The_First_results_of_SMOTE_Classweight_*.csv")

# Process each file individually
for file in result_files:
    # Read the result file into a DataFrame
    df = pd.read_csv(file)

    # Pivot the DataFrame to create a matrix, use the average F1-score as values
    matrix = df.pivot_table(index='SMOTE_ratio', columns='ClassWeight', values='Average_F1_score')

    # Replace the index name with "SMOTE ratios | Class Weights"
    matrix.index.name = "SMOTE ratios | Class Weights"

    # Determine the output filename by replacing "The_First_results_of_SMOTE_Classweight_" with "The_Matrix_sc_result_of_"
    output_file = file.replace('The_First_results_of_SMOTE_Classweight_', 'The_Matrix_sc_result_of_')

    # Save the matrix to a CSV file, replace NaN values with 'NA'
    matrix.to_csv(output_file, na_rep='NA', index=True)






# Get the list of all matrix files
matrix_files = glob.glob(f"{path}The_Matrix_sc_result_of_*.csv")

# Create an empty DataFrame to store the results
results_df = pd.DataFrame(columns=['Matrix_File', 'Max_F1_Score', 'SMOTE_Ratio', 'Class_Weight'])

# Process each matrix file individually
for file in matrix_files:
    # Read the matrix file into a DataFrame
    df = pd.read_csv(file, index_col=0)

    # Find the maximum F1 score and its corresponding SMOTE ratio and class weight
    max_f1 = df.max().max()
    smote_ratio, class_weight = df.stack().idxmax()

    # Append the results to the results DataFrame
    results_df = results_df.append({
        'Matrix_File': file.split('/')[-1],
        'Max_F1_Score': max_f1,
        'SMOTE_Ratio': smote_ratio,
        'Class_Weight': class_weight
    }, ignore_index=True)

# Save the results to a CSV file
results_df.to_csv(f"{path}Updated_sc_Max_F1_Scores_Results.csv", index=False)

# Read the Updated_sc_Max_F1_Scores_Results.csv file
results_df = pd.read_csv(f"{path}Updated_sc_Max_F1_Scores_Results.csv")

# Create a list to store the corresponding .tsv files
tsv_files = []

# For each matrix file in the results DataFrame, identify the corresponding .tsv file
for matrix_file in results_df['Matrix_File']:
    # Extract the relevant part of the matrix file name
    file_part = matrix_file.replace('The_Matrix_sc_result_of_', '').replace('.csv', '')

    # Construct the corresponding .tsv file name
    tsv_file = f"{path}{file_part}.tsv"

    # Check if the .tsv file exists
    if os.path.exists(tsv_file):
        tsv_files.append(tsv_file)
    else:
        print(f"Warning: {tsv_file} does not exist!")
        tsv_files.append(None)
print(tsv_files)

# Load the updated results file
results_df = pd.read_csv('/content/drive/MyDrive/Updated_sc_Max_F1_Scores_Results.csv')

# Define Random Forest Classifier
rf = RandomForestClassifier()

# Store results for each file in a list
results = []

# STEP 1: Extract the corresponding file name and the ratios from each row
for _, row in results_df.iterrows():
    file_name = row['Matrix_File'].replace('The_Matrix_sc_result_of_', '').replace('.csv', '.tsv')
    smote_ratio = row['SMOTE_Ratio']
    class_weight_value = row['Class_Weight']

    # Define class weights
    class_weight = {0: 1, 1: class_weight_value}

    # STEP 2: Run a 10-fold Random Forest using the extracted ratios on the corresponding file
    df = pd.read_csv(f"/content/drive/MyDrive/{file_name}", sep='\t')
    X = df.iloc[:, 1:-1]
    y = df.iloc[:, -1]

    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
    f1_scores = []
    recall_scores = []
    precision_scores = []

    for train_idx, test_idx in skf.split(X, y):
        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

        # Conditionally apply SMOTE
        X_res, y_res = X_train, y_train
        if smote_ratio > 0:
            sm = SMOTE(sampling_strategy=smote_ratio)
            X_res, y_res = sm.fit_resample(X_res, y_res)

        # Train and predict with Random Forest using class weights
        rf = RandomForestClassifier(class_weight=class_weight)
        rf.fit(X_res, y_res)
        predictions = rf.predict(X_test)

        # Store scores
        f1_scores.append(f1_score(y_test, predictions, average='macro'))
        recall_scores.append(recall_score(y_test, predictions, average='macro'))
        precision_scores.append(precision_score(y_test, predictions, average='macro'))

    # STEP 3: Store average scores and file details for each file
    results.append({
        'File_Name': file_name,
        'SMOTE_Ratio': smote_ratio,
        'Class_Weight': class_weight_value,
        'Recall': sum(recall_scores) / len(recall_scores),
        'Precision': sum(precision_scores) / len(precision_scores),
        'F1': sum(f1_scores) / len(f1_scores)
    })

# Convert results to DataFrame and save to CSV
final_results_df = pd.DataFrame(results)
final_results_df.to_csv('/content/drive/MyDrive/Final_sc_Evaluation_Results.csv', index=False)

"""Balanced Random Forest Classifier Evaluation

In this section, we introduce and evaluate a custom ensemble classifier named BalancedRandomForest. This classifier aims to address class imbalance by ensuring that each individual tree in the ensemble is trained on a balanced subset of the data. Specifically:

  Classifier Overview: The BalancedRandomForest is an ensemble of Random Forest classifiers. Each tree within the ensemble is trained on a bootstrap sample that has an equal number of instances from both the minority and majority classes.


  Data Preparation: We'll process datasets stored in a specified directory. Each dataset undergoes preprocessing, including label encoding for categorical features.

  Evaluation Process: Using 10-fold stratified cross-validation, we'll train and evaluate the BalancedRandomForest on each dataset. Performance metrics such as F1-score, precision, and recall are computed for each fold.

  Results: Detailed results, including the number of positive and negative examples used by each tree and the performance metrics, are saved for each dataset. Additionally, we compute and store average results across all folds.
"""

# Define the custom BalancedRandomForest classifier
class BalancedRandomForest(BaseEstimator, ClassifierMixin):
    def __init__(self, n_estimators=100, **kwargs):
        self.n_estimators = n_estimators
        self.forest = [RandomForestClassifier(**kwargs) for _ in range(n_estimators)]

    def fit(self, X, y):
        self.sample_counts = []
        minority_class = min(set(y), key=list(y).count)
        majority_class = max(set(y), key=list(y).count)

        minority_indices = [i for i, value in enumerate(y) if value == minority_class]
        majority_indices = [i for i, value in enumerate(y) if value == majority_class]

        for tree in self.forest:
            minority_bootstrap_sample = resample(minority_indices, replace=True, n_samples=len(minority_indices))
            majority_bootstrap_sample = resample(majority_indices, replace=True, n_samples=len(minority_indices))

            bootstrap_sample = minority_bootstrap_sample + majority_bootstrap_sample
            X_bootstrap = X.iloc[bootstrap_sample]
            y_bootstrap = y.iloc[bootstrap_sample]

            tree.fit(X_bootstrap, y_bootstrap)
            self.sample_counts.append(np.bincount(y_bootstrap))

        return self.sample_counts

    def predict(self, X):
        predictions = [tree.predict(X) for tree in self.forest]
        return np.round(np.mean(predictions, axis=0)).astype(int)

# Specify dataset directory
datasets_path = '/content/drive/MyDrive/'
cv = StratifiedKFold(n_splits=10)
files = [f for f in os.listdir(datasets_path) if f.startswith('ind_') and f.endswith('_frame.tsv')]

for file in files:
    print(f"Processing file: {file}")
    results_df = pd.DataFrame(columns=['NumPosExamp_Original', 'NumNegExamp_Original',
                                       'NumPosExamp_Tree', 'NumNegExamp_Tree',
                                       'F1_score', 'Precision', 'Recall'])

    df = pd.read_csv(os.path.join(datasets_path, file), sep='\t')

    for column in df.columns:
        if df[column].dtype == type(object):
            le = LabelEncoder()
            df[column] = le.fit_transform(df[column])

    X = df[df.columns[:-1]]
    y = df[df.columns[-1]]
    counts_overall = np.bincount(y)

    for train_index, test_index in cv.split(X, y):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        brf = BalancedRandomForest(n_estimators=100)
        sample_counts_tree = brf.fit(X_train, y_train)

        predictions = brf.predict(X_test)
        f1 = f1_score(y_test, predictions, average='macro')
        precision = precision_score(y_test, predictions, average='macro')
        recall = recall_score(y_test, predictions, average='macro')

        for count in sample_counts_tree:
            results_df = results_df.append({
                'NumPosExamp_Original': counts_overall[1],
                'NumNegExamp_Original': counts_overall[0],
                'NumPosExamp_Tree': count[1] if len(count) > 1 else 0,
                'NumNegExamp_Tree': count[0],
                'F1_score': f1,
                'Precision': precision,
                'Recall': recall
            }, ignore_index=True)

        print(f"For {file}, Fold {len(results_df)}, F1-score is {f1}, Precision is {precision}, and Recall is {recall}")

    results_df.to_csv(f'{datasets_path}BRF_results_{file.split(".")[0]}.csv', index=False)

    # Averaging results
    average_results = {
        'NumPosExamp_Original': counts_overall[1],
        'NumNegExamp_Original': counts_overall[0],
        'Average_NumPosExamp_Tree': results_df['NumPosExamp_Tree'].mean(),
        'Average_NumNegExamp_Tree': results_df['NumNegExamp_Tree'].mean(),
        'Average_F1_score': results_df['F1_score'].mean(),
        'Average_Precision': results_df['Precision'].mean(),
        'Average_Recall': results_df['Recall'].mean()
    }

    pd.DataFrame([average_results]).to_csv(f'{datasets_path}BRF_Average_results_{file.split(".")[0]}.csv', index=False)